# https://github.com/ray-project/kuberay/tree/master/helm-chart/ray-cluster

image:
  tag: 2.5.1-py310
fullnameOverride: "ray-cluster"
head:
  nodeSelector:
    node-role.kubernetes.io/control-plane: ""
  resources:
    limits:
      cpu: "14"
      # To avoid out-of-memory issues, never allocate less than 2G memory for the Ray head.
      memory: "10G"
    requests:
      cpu: "14"
      memory: "10G"
  volumeMounts:
    - mountPath: /data
      name: ray-output
  volumes:
    - name: ray-output
      nfs:
        server: master.more2020.eu
        path: /data/1/k8s/forecasting_data/trials
  # Ray writes logs to /tmp/ray/session_latests/logs

worker:
  nodeSelector:
    node-role.kubernetes.io/worker: ""
  resources:
    limits:
      cpu: "14"
      memory: "10G"
    requests:
      cpu: "14"
      memory: "10G"
  volumeMounts:
    - mountPath: /data
      name: ray-output
  volumes:
    - name: ray-output
      nfs:
        server: master.more2020.eu
        path: /data/1/k8s/forecasting_data/trials
        
  replicas: 2
  minReplicas: 2
  maxReplicas: 2